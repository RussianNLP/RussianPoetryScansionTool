# –ò—Å—Ö–æ–¥–Ω–∞—è –≤–µ—Ä—Å–∏—è: –ì—É—Å–µ–≤ –ò–ª—å—è    https://github.com/IlyaGusev/rupo
# –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, —Ñ–∏–∫—Å—ã - kelijah
# –û–ø–∏—Å–∞–Ω–∏–µ: –ú–æ–¥—É–ª—å –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–æ —É–¥–∞—Ä–µ–Ω–∏—è–º –∏ —Å–ª–æ–≥–∞–º.

import json
import os
import xml.etree.ElementTree as etree
from enum import Enum, unique
import re
from collections import OrderedDict
from typing import Set, List, Dict, Tuple
import jsonpickle
import logging

from .emoji import EMOJI_SEQUENCE


CYRRILIC_LOWER_VOWELS = "–∞–æ—ç–∏—É—ã–µ—ë—é—è"
CYRRILIC_LOWER_CONSONANTS = "–π—Ü–∫–Ω–≥—à—â–∑—Ö—ä—Ñ–≤–ø—Ä–ª–¥–∂—á—Å–º—Ç—å–±"
VOWELS = "aeiouAEIOU–∞–æ—ç–∏—É—ã–µ—ë—é—è–ê–û–≠–ò–£–´–ï–Å–Æ–Ø"
CLOSED_SYLLABLE_CHARS = "—Ä–ª–π–º–Ω–†–õ–ô–ú–ù"


#from rupo.settings import HYPHEN_TOKENS
#HYPHEN_TOKENS = resource_filename(__name__, "data/hyphen-tokens.txt")

HYPHEN_TOKENS = [
"-–Ω–∏–±—É–¥—å",
"-–ª–∏–±–æ",
"-–∫–∞–∫",
"-–∫–∞",
"-–Ω–∏–±—É—Ç—å",
"-–≥–¥–µ",
"-—á–µ–≥–æ",
"-—Ç–∞–∫–∏",
"-—á—Ç–æ",
"-–∫–∞–∫–∏–µ",
"-–∫—É–¥–∞",
"-–≥–æ",
"-–Ω–∞",
"-–ø–æ–¥",
"–≤–æ-",
"–≤-",
"–ø–æ-",
"–∫–æ–µ-",
"–∏–∑-",
"–≤–æ—Ç-",
"–Ω—É-",
"–Ω–∞-",
"–Ω–∏-",
"–µ–π-",
"–æ–π-",
"—ç—Ö-",
]




class Token:
    @unique
    class TokenType(Enum):
        """
        –¢–∏–ø —Ç–æ–∫–µ–Ω–∞.
        """
        UNKNOWN = -1
        WORD = 0
        PUNCTUATION = 1
        SPACE = 2
        ENDLINE = 3
        NUMBER = 4

        def __str__(self):
            return str(self.name)

        def __repr__(self):
            return self.__str__()

    def __init__(self, text: str, token_type: TokenType, begin: int, end: int):
        """
        :param text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
        :param token_type: —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞.
        :param begin: –Ω–∞—á–∞–ª–æ –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
        :param end: –∫–æ–Ω–µ—Ü –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
        """
        self.token_type = token_type
        self.begin = begin
        self.end = end
        self.text = text

    def __str__(self):
        return "'" + self.text + "'" + "|" + str(self.token_type) + " (" + str(self.begin) + ", " + str(self.end) + ")"

    def __repr__(self):
        return self.__str__()

    def __eq__(self, other):
        return self.text == other.text and self.token_type == other.token_type


class Tokenizer(object):
    """
    –ö–ª–∞—Å—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.
    """
    @staticmethod
    def tokenize(text: str, remove_punct=False, remove_unknown=False, replace_numbers=False) -> List[Token]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å —É—á—ë—Ç–æ–º –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Å–ª–æ–≤ —Å –¥–µ—Ñ–∏—Å–∞–º–∏.

        :param text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
        :return: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤.
        """
        tokens = []
        punctuation = ".,?:;!‚Äî"
        begin = -1
        for i, ch in enumerate(text):
            if ch.isalpha() or ch == "-":
                if begin == -1:
                    begin = i
            else:
                if begin != -1:
                    tokens.append(Tokenizer.__form_token(text, begin, i))
                    begin = -1
                token_type = Token.TokenType.UNKNOWN
                if ch in punctuation:
                    token_type = Token.TokenType.PUNCTUATION
                elif ch == "\n":
                    token_type = Token.TokenType.ENDLINE
                elif ch == " ":
                    token_type = Token.TokenType.SPACE
                elif ch.isdigit():
                    token_type = Token.TokenType.NUMBER
                if len(tokens) != 0 and tokens[-1].token_type == token_type:
                    tokens[-1].text += ch
                    tokens[-1].end += 1
                else:
                    tokens.append(Token(ch, token_type, i, i + 1))
        if begin != -1:
            tokens.append(Tokenizer.__form_token(text, begin, len(text)))
        tokens = Tokenizer.__hyphen_map(tokens)
        if remove_punct:
            tokens = [token for token in tokens if token.token_type != Token.TokenType.PUNCTUATION]
        if remove_unknown:
            tokens = [token for token in tokens if token.token_type != Token.TokenType.UNKNOWN]
        if replace_numbers:
            for token in tokens:
                if token.token_type != Token.TokenType.NUMBER:
                    continue
                token.text = "–ß–ò–°–õ–û"
                token.token_type = Token.TokenType.WORD
        return tokens

    @staticmethod
    def __form_token(text, begin, end):
        word = text[begin:end]
        if word != "-":
            return Token(word, Token.TokenType.WORD, begin, end)
        else:
            return Token("-", Token.TokenType.PUNCTUATION, begin, begin + 1)

    @staticmethod
    def __hyphen_map(tokens: List[Token]) -> List[Token]:
        """
        –°–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –æ—Å—Ç–∞–≤–ª—è–µ–º —Å –¥–µ—Ñ–∏—Å–æ–º, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—è–µ–º.

        :param tokens: —Ç–æ–∫–µ–Ω—ã.
        :return: —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        """
        new_tokens = []
        hyphen_tokens = Tokenizer.__get_hyphen_tokens()
        for token in tokens:
            if token.token_type != Token.TokenType.WORD:
                new_tokens.append(token)
                continue
            is_one_word = True
            if "-" in token.text:
                is_one_word = False
                for hyphen_token in hyphen_tokens:
                    if hyphen_token in token.text or token.text in hyphen_token:
                        is_one_word = True
            if is_one_word:
                new_tokens.append(token)
            else:
                texts = token.text.split("-")
                pos = token.begin
                for text in texts:
                    new_tokens.append(Token(text, Token.TokenType.WORD, pos, pos+len(text)))
                    pos += len(text) + 1
        return new_tokens

    @staticmethod
    def __get_hyphen_tokens():
        """
        :return: —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–æ–ø–∏—Å–∞–Ω—ã —Å–ª–æ–≤–∞ —Å –¥–µ—Ñ–∏—Å–æ–º.
        """
        #with open(HYPHEN_TOKENS, "r", encoding="utf-8") as file:
        #    hyphen_tokens = [token.strip() for token in file.readlines()]
        #    return hyphen_tokens
        return HYPHEN_TOKENS


class SentenceTokenizer(object):
    @staticmethod
    def tokenize(text: str) -> List[str]:
        m = re.split(r'(?<=[^–ê-–Ø–Å].[^–ê-–Ø–Å][.?!;]) +(?=[–ê-–Ø–Å])', text)
        return m


# =============================================================================

def count_vowels(string):
    num_vowels = 0
    for char in string:
        if char in VOWELS:
            num_vowels += 1
    return num_vowels


def get_first_vowel_position(string):
    for i, ch in enumerate(string):
        if ch in VOWELS:
            return i
    return -1





class Annotation:
    """
    –ö–ª–∞—Å—Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏.
    –°–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—á–∞–ª—å–Ω—É—é –∏ –∫–æ–Ω–µ—á–Ω—É—é –ø–æ–∑–∏—Ü–∏—é –≤ —Ç–µ–∫—Å—Ç–µ, –∞ —Ç–∞–∫–∂–µ —Ç–µ–∫—Å—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏.
    """
    def __init__(self, begin: int, end: int, text: str) -> None:
        self.begin = begin
        self.end = end
        self.text = text


class Syllable(Annotation):
    """
    –†–∞–∑–º–µ—Ç–∫–∞ —Å–ª–æ–≥–∞. –í–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –∏ –Ω–æ–º–µ—Ä —Å–ª–æ–≥–∞, –∞ —Ç–∞–∫–∂–µ —É–¥–∞—Ä–µ–Ω–∏–µ.
    –ï—Å–ª–∏ —É–¥–∞—Ä–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç –Ω–µ –Ω–∞ —ç—Ç–æ—Ç —Å–ª–æ–≥, -1.
    """
    def __init__(self, begin: int, end: int, number: int, text: str, stress: int=-1) -> None:
        super(Syllable, self).__init__(begin, end, text)
        self.number = number
        self.stress = stress

    def vowel(self) -> int:
        """
        :return: –ø–æ–∑–∏—Ü–∏—è –≥–ª–∞—Å–Ω–æ–π –±—É–∫–≤—ã —ç—Ç–æ–≥–æ —Å–ª–æ–≥–∞ –≤ —Å–ª–æ–≤–µ (—Å 0).
        """
        return get_first_vowel_position(self.text) + self.begin

    def from_dict(self, d: dict) -> 'Syllable':
        self.__dict__.update(d)
        if "accent" in self.__dict__:
            self.stress = self.__dict__["accent"]
        return self


def get_syllables(word: str) -> List[Syllable]:
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å–ª–æ–≥–∏.
    :param word: —Å–ª–æ–≤–æ –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ —Å–ª–æ–≥–∏.
    :return syllables: –º–∞—Å—Å–∏–≤ —Å–ª–æ–≥–æ–≤ —Å–ª–æ–≤–∞.
    """
    syllables = []
    begin = 0
    number = 0

    # –í —Å–ª—É—á–∞–µ –Ω–∞–ª–∏—á–∏—è –¥–µ—Ñ–∏—Å–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º —Å–ª–æ–≤–∞ –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞, –Ω–∞—Ö–æ–¥–∏–º —Å–ª–æ–≥–∏ –≤ –Ω–∏—Ö, –æ–±—ä–µ–¥–∏–Ω—è–µ–º.
    if "-" in word:
        if word == "-":
            return [Syllable(0, 1, 0, word)]

        word_parts = word.split("-")
        word_syllables = []
        last_part_end = 0
        for part in word_parts:
            part_syllables = get_syllables(part)
            if len(part_syllables) == 0:
                continue
            for i in range(len(part_syllables)):
                part_syllables[i].begin += last_part_end
                part_syllables[i].end += last_part_end
                part_syllables[i].number += len(word_syllables)
            word_syllables += part_syllables
            last_part_end = part_syllables[-1].end + 1
        return word_syllables

    # –î–ª—è —Å–ª–æ–≤ –∏–ª–∏ –ø–æ–¥—Å–ª–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –¥–µ—Ñ–∏—Å–∞.
    for i, ch in enumerate(word):
        if ch in '0123456789':
            if i>0 and word[i-1] not in '0123456789':
                # –∑–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–ª–æ–≥ –¥–ª—è —Å–ª—É—á–∞–µ–≤ x1234
                syllables.append(Syllable(begin, i, number, word[begin:i]))
                number += 1

            syllables.append(Syllable(i, i+1, number, word[i:i+1]))
            number += 1
            begin = i+1
            continue
        elif re.match(rf'^([_]|{EMOJI_SEQUENCE})$', ch) is not None:
            # –í—Å—è–∫–∏–µ —ç–º–æ–¥–∑–∏, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –≤—ã–¥–µ–ª—è—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–ª–æ–≥.
            if begin != i:
                # –∑–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–ª–æ–≥ –¥–ª—è —Å–ª—É—á–∞—è —á–µ–ª–æ–≤–µ–∫üôà
                syllables.append(Syllable(begin, i, number, word[begin:i]))
                begin = i

            syllables.append(Syllable(i, i+1, number, word[i:i+1]))
            number += 1
            begin = i+1
            continue
        elif ch not in VOWELS:
            continue
        elif i + 1 < len(word) - 1 and word[i + 1] in CLOSED_SYLLABLE_CHARS:
            if i + 2 < len(word) - 1 and word[i + 2] in "—å–¨":
                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Å–æ–Ω–æ—Ä–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–Ω–æ–≥–æ –∏–¥—ë—Ç –º—è–≥–∫–∏–π –∑–Ω–∞–∫, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –Ω–∞ –Ω—ë–º. ("–±–∞–Ω—å-–∫–∞")
                end = i + 3
            elif i + 2 < len(word) - 1 and word[i + 2] not in VOWELS and \
                    (word[i + 2] not in CLOSED_SYLLABLE_CHARS or word[i + 1] == "–π"):
                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Å–æ–Ω–æ—Ä–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–Ω–æ–≥–æ –Ω–µ –∏–¥—ë—Ç –≥–ª–∞—Å–Ω–∞—è –∏–ª–∏ –¥—Ä—É–≥–æ–π —Å–æ–Ω–æ—Ä–Ω—ã–π —Å–æ–≥–ª–∞—Å–Ω—ã–π,
                # —Å–ª–æ–≥ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç–æ–º —Å–æ–≥–ª–∞—Å–Ω–æ–º. ("–º–∞–π-–∫–∞")
                end = i + 2
            else:
                # –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–≥–æ —Å–æ–≥–ª–∞—Å–Ω–æ–≥–æ, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –Ω–∞ –≥–ª–∞—Å–Ω–æ–π.
                # ("—Å–æ-–ª–æ", "–¥–∞-–Ω–Ω—ã–π", "–ø–æ–ª-–Ω—ã–π")
                end = i + 1
        else:
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –≥–ª–∞—Å–Ω–æ–π –∏–¥—ë—Ç –Ω–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–æ–≥–ª–∞—Å–Ω–∞—è, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –Ω–∞ –≥–ª–∞—Å–Ω–æ–π. ("–∫–æ-–≥–¥–∞")
            end = i + 1

        syllables.append(Syllable(begin, end, number, word[begin:end]))
        number += 1
        begin = end

    if get_first_vowel_position(word) != -1:
        # –î–æ–±–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–≥ –¥–æ –∫–æ–Ω—Ü–∞ —Å–ª–æ–≤–∞.
        syllables[-1] = Syllable(syllables[-1].begin, len(word), syllables[-1].number, word[syllables[-1].begin:len(word)])

    # 05.04.2022
    # –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è –æ–¥–Ω–æ–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ (–ø—Ä–µ–¥–ª–æ–≥–∏, —á–∞—Å—Ç–∏—Ü—ã) –∏–∑ —Å–æ–≥–ª–∞—Å–Ω–æ–π, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞—é—Ç 0 —Å–ª–æ–≥–æ–≤.
    if len(syllables) == 0:
        syllables.append(Syllable(begin=0, end=1, number=0, text=word))

    return syllables




class Word(Annotation):
    """
    –†–∞–∑–º–µ—Ç–∫–∞ —Å–ª–æ–≤–∞. –í–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å–ª–æ–≤–∞ –∏ –µ–≥–æ —Å–ª–æ–≥–∏.
    """
    def __init__(self, begin: int, end: int, text: str, syllables: List[Syllable]) -> None:
        super(Word, self).__init__(begin, end, text)
        self.syllables = syllables

    def count_stresses(self) -> int:
        """
        :return: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞—Ä–µ–Ω–∏–π –≤ —Å–ª–æ–≤–µ.
        """
        return sum(syllable.stress != -1 for syllable in self.syllables)

    def stress(self) -> int:
        """
        :return: –ø–æ—Å–ª–µ–¥–Ω–µ–µ —É–¥–∞—Ä–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–µ, –µ—Å–ª–∏ –Ω–µ—Ç, —Ç–æ -1.
        """
        stress = -1
        for syllable in self.syllables:
            if syllable.stress != -1:
                stress = syllable.stress
        return stress

    def get_stressed_syllables_numbers(self) -> List[int]:
        """
        :return: –Ω–æ–º–µ—Ä–∞ —Å–ª–æ–≥–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø–∞–¥–∞—é—Ç —É–¥–∞—Ä–µ–Ω–∏—è.
        """
        return [syllable.number for syllable in self.syllables if syllable.stress != -1]

    def get_stresses(self) -> Set[int]:
        """
        :return: –≤—Å–µ —É–¥–∞—Ä–µ–Ω–∏—è.
        """
        stresses = set()
        for syllable in self.syllables:
            if syllable.stress != -1:
                stresses.add(syllable.stress)
        return stresses

    def set_stresses(self, stresses: List[int]) -> None:
        """
        –ó–∞–¥–∞—Ç—å —É–¥–∞—Ä–µ–Ω–∏—è, –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É–±–∏—Ä–∞—é—Ç—Å—è.

        :param stresses: –ø–æ–∑–∏—Ü–∏–∏ —É–¥–∞—Ä–µ–Ω–∏—è –≤ —Å–ª–æ–≤–µ.
        """
        for syllable in self.syllables:
            if syllable.vowel() in stresses:
                syllable.stress = syllable.vowel()
            else:
                syllable.stress = -1

    def get_short(self) -> str:
        """
        :return: —Å–ª–æ–≤–æ –≤ —Ñ–æ—Ä–º–µ "—Ç–µ–∫—Å—Ç"+"–ø–æ—Å–ª–µ–¥–Ω–µ–µ —É–¥–∞—Ä–µ–Ω–∏–µ".
        """
        return self.text.lower() + str(self.stress())

    def from_dict(self, d: dict) -> 'Word':
        self.__dict__.update(d)
        syllables = d["syllables"]  # type: List[dict]
        self.syllables = [Syllable(0, 0, 0, "").from_dict(syllable) for syllable in syllables]
        return self

    def to_stressed_word(self):
        from rupo.stress.word import StressedWord, Stress
        return StressedWord(self.text, set([Stress(pos, Stress.Type.PRIMARY) for pos in self.get_stresses()]))

    def __hash__(self) -> int:
        """
        :return: —Ö–µ—à —Ä–∞–∑–º–µ—Ç–∫–∏.
        """
        return hash(self.get_short())


class Line(Annotation):
    """
    –†–∞–∑–º–µ—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏. –í–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å—Ç—Ä–æ–∫–∏ –∏ –µ—ë —Å–ª–æ–≤–∞.
    """
    def __init__(self, begin: int, end: int, text: str, words: List[Word]) -> None:
        super(Line, self).__init__(begin, end, text)
        self.words = words

    def from_dict(self, d) -> 'Line':
        self.__dict__.update(d)
        words = d["words"]  # type: List[dict]
        self.words = [Word(0, 0, "", []).from_dict(word) for word in words]
        return self

    def count_vowels(self):
        num_vowels = 0
        for word in self.words:
            for syllable in word.syllables:
                if get_first_vowel_position(syllable.text) != -1:
                    num_vowels += 1
        return num_vowels


class Markup: #(CommonMixin):
    """
    –ö–ª–∞—Å—Å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –≤ —Ü–µ–ª–æ–º —Å —ç–∫—Å–ø–æ—Ä—Ç–æ–º/–∏–º–ø–æ—Ä—Ç–æ–º –≤ XML –∏ JSON.
    """
    def __init__(self, text: str=None, lines: List[Line]=None) -> None:
        self.text = text
        self.lines = lines
        self.version = 2

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False)

    def from_json(self, st) -> 'Markup':
        d = json.loads(st)
        return self.from_dict(d)

    def from_dict(self, d) -> 'Markup':
        self.__dict__.update(d)
        lines = d["lines"]  # type: List[dict]
        self.lines = [Line(0, 0, "", []).from_dict(line) for line in lines]
        return self

    def to_xml(self) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –≤ XML.

        :return self: —Å—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XML
        """
        pass #return dicttoxml(self.to_dict(), custom_root='markup', attr_type=False).decode('utf-8').replace("\n", "\\n")

    def from_xml(self, xml: str) -> 'Markup':
        """
        –ò–º–ø–æ—Ä—Ç –∏–∑ XML.

        :param xml: XML-—Ä–∞–∑–º–µ—Ç–∫–∞
        :return self: –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è –æ–±—ä–µ–∫—Ç Markup
        """
        root = etree.fromstring(xml)
        if root.find("version") is None or int(root.find("version").text) != self.version:
            raise TypeError("–î—Ä—É–≥–∞—è –≤–µ—Ä—Å–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏")
        lines_node = root.find("lines")
        lines = []
        for line_node in lines_node.findall("item"):
            words_node = line_node.find("words")
            words = []
            for word_node in words_node.findall("item"):
                syllables_node = word_node.find("syllables")
                syllables = []
                for syllable_node in syllables_node.findall("item"):
                    stress_node = syllable_node.find("accent") \
                        if syllable_node.find("accent") is not None \
                        else syllable_node.find("stress")
                    stress = int(stress_node.text)
                    syllables.append(Syllable(int(syllable_node.find("begin").text),
                                              int(syllable_node.find("end").text),
                                              int(syllable_node.find("number").text),
                                              syllable_node.find("text").text,
                                              stress))
                words.append(Word(int(word_node.find("begin").text), int(word_node.find("end").text),
                                  word_node.find("text").text, syllables))
            lines.append(Line(int(line_node.find("begin").text), int(line_node.find("end").text),
                              line_node.find("text").text, words))
        self.text = root.find("text").text.replace("\\n", "\n")
        self.lines = lines
        return self

    def from_raw(self, text: str) -> 'Markup':
        """
        –ò–º–ø–æ—Ä—Ç –∏–∑ —Å—ã—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —É–¥–∞—Ä–µ–Ω–∏—è–º–∏ –≤ –∫–æ–Ω—Ü–µ —Å–ª–æ–≤

        :param text: —Ç–µ–∫—Å—Ç.
        :return: —Ä–∞–∑–º–µ—Ç–∫–∞.
        """

        pos = 0
        lines = []
        for line in text.split("\n"):
            if line == "":
                continue
            line_tokens = []
            for word in line.split(" "):
                i = -1
                ch = word[i]
                stress = ""
                while ch.isdigit() or ch == "-":
                    stress += ch
                    i -= 1
                    ch = word[i]
                line_tokens.append((word[:i+1], int(stress[::-1])))
            words = []
            line_begin = pos
            for pair in line_tokens:
                token = pair[0]
                stress = pair[1]
                syllables = get_syllables(token)
                for j in range(len(syllables)):
                    syllables[j].begin += pos
                    syllables[j].end += pos
                word = Word(pos, pos + len(token), token, syllables)
                word.set_stresses([stress])
                words.append(word)
                pos += len(token) + 1
            lines.append(Line(line_begin, pos, " ".join([pair[0] for pair in line_tokens]), words))
        self.text = "\n".join([line.text for line in lines])
        self.lines = lines
        return self

    @staticmethod
    def process_text(text: str, stress_predictor) -> 'Markup':
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–æ —Å–ª–æ–≥–∞–º –∏ —É–¥–∞—Ä–µ–Ω–∏—è–º.

        :param text: —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏
        :param stress_predictor: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —É–¥–∞—Ä–µ–Ω–∏–π.
        :return markup: —Ä–∞–∑–º–µ—Ç–∫–∞ –ø–æ —Å–ª–æ–≥–∞–º –∏ —É–¥–∞—Ä–µ–Ω–∏—è–º
        """
        begin_line = 0
        lines = []
        words = []
        text_lines = text.split("\n")
        for text_line in text_lines:
            tokens = [token for token in Tokenizer.tokenize(text_line) if token.token_type == Token.TokenType.WORD]
            for token in tokens:
                word = Word(begin_line + token.begin, begin_line + token.end, token.text, get_syllables(token.text))
                # –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º —É–¥–∞—Ä–µ–Ω–∏—è.
                stresses = stress_predictor.predict(token.text.lower())
                # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —É–¥–∞—Ä–µ–Ω–∏—è —Å–ª–æ–≥–∞–º.
                if len(word.syllables) > 1:
                    word.set_stresses(stresses)
                words.append(word)
            end_line = begin_line + len(text_line)
            lines.append(Line(begin_line, end_line, text_line, words))
            words = []
            begin_line = end_line + 1
        return Markup(text, lines)


# ==================================================================================


class TreeNode:
    """
    –ù–æ–¥–∞ –¥–µ—Ä–µ–≤–∞ —Ä–∞–∑–±–æ—Ä–∞ —à–∞–±–ª–æ–Ω–∞.
    """
    leaf_chars = "usUS"
    non_leaf_chars = "*?w"

    def __init__(self, parent: 'TreeNode', children: List['TreeNode'], text: str, pattern_pos: int):
        """
        :param parent: —Ä–æ–¥–∏—Ç–µ–ª—å –Ω–æ–¥—ã.
        :param children: –¥–µ—Ç–∏ –Ω–æ–¥—ã.
        :param text: —Å–∏–º–≤–æ–ª, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –Ω–æ–¥–µ.
        :param pattern_pos: –ø–æ–∑–∏—Ü–∏—è —Å–∏–º–≤–æ–ª–∞ –≤ —à–∞–±–ª–æ–Ω–µ
        """
        self.parent = parent  # type: TreeNode
        self.children = children  # type: List[TreeNode]
        self.text = text  # type: str
        self.pattern_pos = pattern_pos  # type: int

    def get_level(self) -> int:
        """
        :return: –≤—ã—Å–æ—Ç–∞ –Ω–æ–¥—ã –≤ –¥–µ—Ä–µ–≤–µ.
        """
        parent = self.parent
        level = 0
        while parent is not None:
            parent = parent.parent
            level += 1
        return level

    def get_next_sibling(self) -> 'TreeNode':
        """
        :return: —Å–æ—Å–µ–¥–Ω—è—è –Ω–æ–¥–∞ —Å–ø—Ä–∞–≤–∞.
        """
        siblings = self.parent.children
        index = siblings.index(self) + 1
        if index < len(siblings):
            return siblings[index]
        return None

    def get_last_child_leaf(self) -> 'TreeNode':
        """
        :return: –ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–æ–¥–∞ –∏–∑–µ –¥–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –ª–∏—Å—Ç–æ–º.
        """
        for child in reversed(self.children):
            if child.is_leaf():
                return child
        return None

    def is_first_leaf(self) -> bool:
        if not self.is_leaf():
            return False
        return [child for child in self.parent.children if child.is_leaf()][0] == self

    def is_last_leaf(self) -> bool:
        if not self.is_leaf():
            return False
        return [child for child in self.parent.children if child.is_leaf()][-1] == self

    def get_most_left_leaf(self) -> 'TreeNode':
        """
        :return: —Å–∞–º—ã–π –ª–µ–≤—ã–π –ø–æ—Ç–æ–º–æ–∫.
        """
        node = self
        while len(node.children) != 0:
            node = node.children[0]
        assert node.is_leaf()
        return node

    def print_tree(self) -> None:
        """
        –í—ã–≤–æ–¥ –¥–µ—Ä–µ–≤–∞ —Å –∫–æ—Ä–Ω–µ–º –≤ —ç—Ç–æ–π –Ω–æ–¥–µ.
        """
        stack = list()
        stack.append(self)
        while len(stack) != 0:
            current_node = stack.pop()
            print("\t" * current_node.get_level(), current_node)
            stack += current_node.children

    def is_leaf(self) -> bool:
        """
        :return: —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–¥–∞ –ª–∏—Å—Ç–æ–º –¥–µ—Ä–µ–≤–∞.
        """
        return self.text in TreeNode.leaf_chars

    def __str__(self) -> str:
        return self.text + " " + str(self.pattern_pos)

    def __repr__(self) -> str:
        return self.__str__()

    def __hash__(self):
        return hash(self.pattern_pos)

    def __eq__(self, other):
        return self.pattern_pos == other.pattern_pos


class State:
    """
    –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–∞–∑–±–æ—Ä–∞.
    """

    def __init__(self, node: TreeNode, string_pos: int, strong_errors: int, weak_errors: int, pattern: str):
        """
        :param node: –Ω–æ–¥–∞ –¥–µ—Ä–µ–≤–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è —Å–æ—Å—Ç–æ—è–Ω–∏—é.
        :param string_pos: –ø–æ–∑–∏—Ü–∏—è –≤ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º–æ–π —Å—Ç—Ä–æ–∫–µ.
        :param strong_errors: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –≤ U –∏ S.
        :param weak_errors: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –≤ u –∏ s.
        :param pattern: —à–∞–±–ª–æ–Ω - –ø—É—Ç—å, –¥–æ —ç—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.
        """
        self.node = node  # type: TreeNode
        self.string_pos = string_pos  # type: int
        self.strong_errors = strong_errors  # type: int
        self.weak_errors = weak_errors  # type: int
        self.pattern = pattern  # type: str

    def __str__(self) -> str:
        return str(self.node) + " " + str(self.string_pos) + " " + str(self.strong_errors) + " " + str(self.weak_errors)

    def __repr__(self) -> str:
        return self.__str__()


class PatternAnalyzer:
    """
    –°–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç–µ–ª—å —à–∞–±–ª–æ–Ω–∞ –∏ —Å—Ç—Ä–æ–∫–∏.
    """

    def __init__(self, pattern: str, error_border: int = 8):
        """
        :param error_border: –≥—Ä–∞–Ω–∏—Ü–∞ –ø–æ –æ—à–∏–±–∫–∞–º.
        :param pattern: —à–∞–±–ª–æ–Ω.
        """
        self.pattern = pattern  # type: str
        self.tree = self.__build_tree(pattern)  # type: TreeNode
        self.error_border = error_border

    @staticmethod
    def count_errors(pattern: str, string: str, error_border: int = 8) -> Tuple[str, int, int, bool]:
        """
        :param pattern: —à–∞–±–ª–æ–Ω.
        :param string: —Å—Ç—Ä–æ–∫–∞.
        :param error_border: –≥—Ä–∞–Ω–∏—Ü–∞ –ø–æ –æ—à–∏–±–∫–∞–º.
        :return: –ª—É—á—à–∏–π —à–∞–±–ª–æ–Ω, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–∞–±—ã—Ö –æ—à–∏–±–æ–∫.
        """
        analyzer = PatternAnalyzer(pattern, error_border)
        return analyzer.__accept(string)

    @staticmethod
    def __build_tree(pattern: str) -> TreeNode:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ —à–∞–±–ª–æ–Ω–∞.

        :param pattern: —à–∞–±–ª–æ–Ω.
        :return: –∫–æ—Ä–µ–Ω—å –¥–µ—Ä–µ–≤–∞.
        """
        root_node = TreeNode(None, list(), "R", -1)
        current_node = root_node
        for i, ch in enumerate(pattern):
            if ch == "(":
                node = TreeNode(current_node, list(), "()", i)
                current_node.children.append(node)
                current_node = node
            if ch == ")":
                node = current_node
                current_node = current_node.parent
                # –£–±–∏—Ä–∞–µ–º –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏.
                if i + 1 < len(pattern) and pattern[i + 1] not in "*?":
                    current_node.children = current_node.children[:-1] + node.children
                    for child in node.children:
                        child.parent = current_node
            if ch in TreeNode.leaf_chars:
                current_node.children.append(TreeNode(current_node, list(), ch, i))
            # –ó–∞–º–µ–Ω—è–µ–º —Å–∫–æ–±–∫–∏ –Ω–∞ –Ω–µ—Ç–µ—Ä–º–∏–Ω–∞–ª—ã.
            if ch in TreeNode.non_leaf_chars:
                current_node.children[-1].text = ch
                current_node.children[-1].pattern_pos = i
        return root_node

    def __accept(self, string: str) -> Tuple[str, int, int, bool]:
        """
        :param string: —Å—Ç—Ä–æ–∫–∞.
        :return: –ª—É—á—à–∏–π —à–∞–±–ª–æ–Ω, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–∞–±—ã—Ö –æ—à–∏–±–æ–∫, –±—ã–ª–∏ –ª–∏ –æ—à–∏–±–∫–∏.
        """
        current_states = [State(None, -1, 0, 0, "")]
        current_node = self.tree.get_most_left_leaf()
        for i, ch in enumerate(string):
            new_states = []
            for state in current_states:
                if state.node is not None:
                    current_node = self.__get_next_leaf(state.node)
                variants = self.__get_variants(current_node)

                # –ö–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.
                for variant in variants:
                    assert variant.is_leaf()
                    strong_errors = state.strong_errors + int(variant.text.isupper() and variant.text != ch)
                    weak_errors = state.weak_errors + int(variant.text.islower() and variant.text != ch.lower())
                    new_state = State(variant, i, strong_errors, weak_errors, state.pattern + variant.text)
                    if new_state.strong_errors + new_state.weak_errors > self.error_border:
                        continue
                    new_states.append(new_state)

            if len(new_states) == 0:
                # –ú–æ–∂–µ–º –∑–∞–∫–æ–Ω—á–∏—Ç—å —Ä–∞–Ω—å—à–µ, –µ—Å–ª–∏ –ø–æ –æ—à–∏–±–∫–∞–º –ø–æ—Ä–µ–∑–∞–ª–∏ –≤–µ—Ç–∫–∏, –ª–∏–±–æ –µ—Å–ª–∏ —à–∞–±–ª–æ–Ω –º–µ–Ω—å—à–µ —Å—Ç—Ä–æ–∫–∏.
                current_states = PatternAnalyzer.__filter_states(current_states, self.tree)
                pattern, strong_errors, weak_errors = self.__get_min_errors_from_states(current_states)
                diff = (len(string) - i)
                return pattern, strong_errors + diff, weak_errors + diff, True

            current_states = new_states
        current_states = PatternAnalyzer.__filter_states(current_states, self.tree)
        return self.__get_min_errors_from_states(current_states) + (False,)

    @staticmethod
    def __get_variants(current_node: TreeNode) -> Set[TreeNode]:
        """
        :param current_node: —Ç–µ–∫—É—â–∞—è –Ω–æ–¥–∞.
        :return: –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–¥—ã –Ω–∞ —Ç–æ–º –∂–µ —Å–∏–º–≤–æ–ª–µ —Å—Ç—Ä–æ–∫–∏, –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∏–∑-–∑–∞ * –∏ ? –≤ —à–∞–±–ª–æ–Ω–µ.
        """
        variants = set()
        current_variant = current_node
        while current_variant is not None:
            if current_variant not in variants:
                variants.add(current_variant)
            else:
                current_variant = current_variant.parent
            current_variant = PatternAnalyzer.__get_next_variant(current_variant)
        return variants

    @staticmethod
    def __get_next_variant(node: TreeNode) -> TreeNode:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏–∑ –≤–∞—Ä–∏–Ω–∞—Ç–æ–≤ —Ç–µ–∫—É—â–µ–π –Ω–æ–¥—ã.

        :param node: —Ç–µ–∫—É—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç.
        :return: —Å–ª–µ–¥—É—é—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç.
        """
        assert node.is_leaf()
        while node.parent is not None:
            parent = node.parent
            grandfather = parent.parent
            uncle = parent.get_next_sibling() if grandfather is not None else None
            is_variable = node.is_first_leaf() or not node.is_leaf()
            if is_variable and uncle is not None:
                return uncle.get_most_left_leaf()
            elif grandfather is not None and grandfather.text == "*" and grandfather.children[-1] == parent:
                return grandfather.get_most_left_leaf()
            if is_variable:
                node = parent
            else:
                break
        return None

    @staticmethod
    def __get_next_leaf(node: TreeNode) -> TreeNode:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–π –Ω–æ–¥—ã.

        :param node: —Ç–µ–∫—É—â–∞—è –Ω–æ–¥–∞.
        :return: —Å–ª–µ–¥—É—é—â–∞—è –Ω–æ–¥–∞.
        """
        assert node.is_leaf()
        while node.parent is not None:
            sibling = node.get_next_sibling()
            if sibling is not None:
                return sibling.get_most_left_leaf()
            elif node.parent.text == "*":
                return node.parent.get_most_left_leaf()
            node = node.parent
        return None

    @staticmethod
    def __filter_states(states: List[State], root: TreeNode) -> List[State]:
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–ª–∏—á–∏—é –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.

        :param states: —Å–æ—Å—Ç–æ—è–Ω–∏—è.
        :param root: –∫–æ—Ä–µ–Ω—å –¥–µ—Ä–µ–≤–∞.
        :return: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è.
        """
        return [state for state in states if root.get_last_child_leaf() is None or
                state.node.pattern_pos >= root.get_last_child_leaf().pattern_pos]

    @staticmethod
    def __get_min_errors_from_states(states: List[State]) -> Tuple[str, int, int]:
        """
        :param states: —Å–æ—Å—Ç–æ—è–Ω–∏—è.
        :return: –ª—É—á—à–∏–π —à–∞–±–ª–æ–Ω, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–∞–±—ã—Ö –æ—à–∏–±–æ–∫.
        """
        if len(states) == 0:
            return "", 0, 0
        return min([(state.pattern, state.strong_errors, state.weak_errors) for i, state in enumerate(states)],
                   key=lambda x: (x[1], x[2], x[0]))


# =====================================================================================


class StressCorrection: #(CommonMixin):
    """
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É–¥–∞—Ä–µ–Ω–∏—è.
    """
    def __init__(self, line_number: int, word_number: int, syllable_number: int,
                 word_text: str, stress: int) -> None:
        """
        :param line_number: –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏.
        :param word_number: –Ω–æ–º–µ—Ä —Å–ª–æ–≤–∞.
        :param syllable_number: –Ω–æ–º–µ—Ä —Å–ª–æ–≥–∞.
        :param word_text: —Ç–µ–∫—Å—Ç —Å–ª–æ–≤–∞.
        :param stress: –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è (—Å 0).
        """
        self.line_number = line_number
        self.word_number = word_number
        self.syllable_number = syllable_number
        self.word_text = word_text
        self.stress = stress


class ClassificationResult: #(CommonMixin):
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è –ø–æ –º–µ—Ç—Ä—É.
    """
    def __init__(self, count_lines: int=0) -> None:
        """
        :param count_lines: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫.
        """
        self.metre = None
        self.count_lines = count_lines
        self.errors_count = {k: 0 for k in MetreClassifier.metres.keys()}  # type: Dict[str, int]
        self.corrections = {k: [] for k in MetreClassifier.metres.keys()}  # type: Dict[str, List[StressCorrection]]
        self.resolutions = {k: [] for k in MetreClassifier.metres.keys()}  # type: Dict[str, List[StressCorrection]]
        self.additions = {k: [] for k in MetreClassifier.metres.keys()}  # type: Dict[str, List[StressCorrection]]

    def get_metre_errors_count(self):
        """
        :return: –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –º–µ—Ç—Ä–µ.
        """
        return self.errors_count[self.metre]

    def to_json(self):
        """
        :return: —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ json.
        """
        return jsonpickle.encode(self)

    @staticmethod
    def str_corrections(collection: List[StressCorrection]) -> str:
        """
        :param collection: —Å–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π.
        :return: –µ–≥–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.
        """
        return"\n".join([str((item.word_text, item.syllable_number)) for item in collection])

    def __str__(self):
        st = "–ú–µ—Ç—Ä: " + str(self.metre) + "\n"
        st += "–°–Ω—è—Ç–∞—è –æ–º–æ–≥—Ä–∞—Ñ–∏—è: \n" + ClassificationResult.str_corrections(self.resolutions[self.metre]) + "\n"
        st += "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —É–¥–∞—Ä–µ–Ω–∏—è: \n" + ClassificationResult.str_corrections(self.corrections[self.metre]) + "\n"
        st += "–ù–æ–≤—ã–µ —É–¥–∞—Ä–µ–Ω–∏—è: \n" + ClassificationResult.str_corrections(self.additions[self.metre]) + "\n"
        return st


class ErrorsTableRecord:
    def __init__(self, strong_errors, weak_errors, pattern, failed=False):
        self.strong_errors = strong_errors
        self.weak_errors = weak_errors
        self.pattern = pattern
        self.failed = failed

    def __str__(self):
        return self.pattern + " " + str(self.strong_errors) + " " + str(self.weak_errors)

    def __repr__(self):
        return self.__str__()


class ErrorsTable:
    def __init__(self, num_lines):
        self.data = {}
        self.num_lines = num_lines
        self.coef = OrderedDict(
            [("iambos", 0.3),
             ("choreios", 0.3),
             ("daktylos", 0.4),
             ("amphibrachys", 0.4),
             ("anapaistos", 0.4),
             ("dolnik3", 0.5),
             ("dolnik2", 0.5),
             ("taktovik3", 6.0),
             ("taktovik2", 6.0)
             ])
        self.sum_coef = OrderedDict(
            [("iambos", 0.0),
             ("choreios", 0.0),
             ("daktylos", 0.0),
             ("amphibrachys", 0.0),
             ("anapaistos", 0.0),
             ("dolnik3", 0.035),
             ("dolnik2", 0.035),
             ("taktovik3", 0.10),
             ("taktovik2", 0.10)
             ])
        for metre_name in MetreClassifier.metres.keys():
            self.data[metre_name] = [ErrorsTableRecord(0, 0, "") for _ in range(num_lines)]

    def add_record(self, metre_name, line_num, strong_errors, weak_errors, pattern, failed=False):
        self.data[metre_name][line_num] = ErrorsTableRecord(strong_errors, weak_errors, pattern, failed)

    def get_best_metre(self):
        for l in range(self.num_lines):
            strong_sum = 0
            weak_sum = 0
            for metre_name in self.data.keys():
                strong_sum += self.data[metre_name][l].strong_errors
                weak_sum += self.data[metre_name][l].weak_errors
            for metre_name, column in self.data.items():
                if strong_sum != 0:
                    column[l].strong_errors = column[l].strong_errors / float(strong_sum)
                if weak_sum != 0:
                    column[l].weak_errors = column[l].weak_errors / float(weak_sum)
        sums = dict()
        for metre_name in self.data.keys():
            sums[metre_name] = (0, 0)
        for metre_name, column in self.data.items():
            strong_sum = 0
            weak_sum = 0
            for l in range(self.num_lines):
                strong_sum += column[l].strong_errors
                weak_sum += column[l].weak_errors
            sums[metre_name] = (strong_sum, weak_sum)
        for metre_name, pair in sums.items():
            sums[metre_name] = self.sum_coef[metre_name] + (pair[0] + pair[1] / 2.0) * self.coef[metre_name] / self.num_lines
        logging.debug(sums)
        return min(sums, key=sums.get)


class MetreClassifier(object):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, —Å—á–∏—Ç–∞–µ—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ —Ä–∏—Ç–º–∞(–º–µ—Ç—Ä–æ–≤).
    """
    metres = OrderedDict(
        [("iambos", '(us)*(uS)(U)?(U)?'),
         ("choreios", '(su)*(S)(U)?(U)?'),
         ("daktylos", '(suu)*(S)(U)?(U)?'),
         ("amphibrachys", '(usu)*(uS)(U)?(U)?'),
         ("anapaistos",  '(uus)*(uuS)(U)?(U)?'),
         ("dolnik3", '(u)?(u)?((su)(u)?)*(S)(U)?(U)?'),
         ("dolnik2", '(u)?(u)?((s)(u)?)*(S)(U)?(U)?'),
         ("taktovik3", '(u)?(u)?((su)(u)?(u)?)*(S)(U)?(U)?'),
         ("taktovik2", '(u)?(u)?((s)(u)?(u)?)*(S)(U)?(U)?')
         ])

    border_syllables_count = 20

    @staticmethod
    def classify_metre(markup):
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–Ω—ã–π –º–µ—Ç—Ä.

        :param markup: —Ä–∞–∑–º–µ—Ç–∫–∞.
        :return: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        """
        result = ClassificationResult(len(markup.lines))
        num_lines = len(markup.lines)
        errors_table = ErrorsTable(num_lines)
        for l, line in enumerate(markup.lines):
            for metre_name, metre_pattern in MetreClassifier.metres.items():
                line_syllables_count = sum([len(word.syllables) for word in line.words])

                # –°—Ç—Ä–æ—á–∫–∏ –¥–ª–∏–Ω–æ–π –±–æ–ª—å—à–µ border_syllables_count —Å–ª–æ–≥–æ–≤ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º.
                if line_syllables_count > MetreClassifier.border_syllables_count or line_syllables_count == 0:
                    continue
                error_border = 7
                if metre_name == "dolnik2" or metre_name == "dolnik3":
                    error_border = 3
                if metre_name == "taktovik2" or metre_name == "taktovik3":
                    error_border = 2
                pattern, strong_errors, weak_errors, analysis_errored = \
                    PatternAnalyzer.count_errors(MetreClassifier.metres[metre_name],
                                                 MetreClassifier.__get_line_pattern(line),
                                                 error_border)
                if analysis_errored or len(pattern) == 0:
                    errors_table.add_record(metre_name, l, strong_errors, weak_errors, pattern, True)
                    continue
                corrections = MetreClassifier.__get_line_pattern_matching_corrections(line, l, pattern)[0]
                accentuation_errors = len(corrections)
                strong_errors += accentuation_errors
                errors_table.add_record(metre_name, l, strong_errors, weak_errors, pattern)
        result.metre = errors_table.get_best_metre()

        # –ó–∞–ø–æ–º–Ω–∏–º –≤—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
        for l, line in enumerate(markup.lines):
            pattern = errors_table.data[result.metre][l].pattern
            failed = errors_table.data[result.metre][l].failed
            if failed or len(pattern) == 0:
                continue
            corrections, resolutions, additions =\
                MetreClassifier.__get_line_pattern_matching_corrections(line, l, pattern)
            result.corrections[result.metre] += corrections
            result.resolutions[result.metre] += resolutions
            result.additions[result.metre] += additions
            result.errors_count[result.metre] += len(corrections)
        return result

    @staticmethod
    def __get_line_pattern(line: Line) -> str:
        """
        –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É —à–∞–±–ª–æ–Ω—É, —Å—á–∏—Ç–∞–µ–º –æ—à–∏–±–∫–∏.

        :param line: —Å—Ç—Ä–æ–∫–∞.
        :return: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
        """
        pattern = ""
        for w, word in enumerate(line.words):
            if len(word.syllables) == 0:
                pattern += "U"
            else:
                for syllable in word.syllables:
                    if syllable.stress != -1:
                        pattern += "S"
                    else:
                        pattern += "U"
        return pattern

    @staticmethod
    def __get_line_pattern_matching_corrections(line: Line, line_number: int, pattern: str) \
            -> Tuple[List[StressCorrection], List[StressCorrection], List[StressCorrection]]:
        """
        –£–¥–∞—Ä–µ–Ω–∏—è –º–æ–≥—É—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ —Å–ª–∞–±–æ–µ –º–µ—Å—Ç–æ,
        –µ—Å–ª–∏ –±–µ–∑—É–¥–∞—Ä–Ω—ã–π —Å–ª–æ–≥ —Ç–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞ –Ω–µ –ø–æ–ø–∞–¥–∞–µ—Ç –Ω–∞ –∏–∫—Ç. –ò–Ω–∞—á–µ - –æ—à–∏–±–∫–∞.

        :param line: —Å—Ç—Ä–æ–∫–∞.
        :param line_number: –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏.
        :param pattern: —à–∞–±–ª–æ–Ω.
        :return: –æ—à–∏–±–∫–∏, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Å–Ω—è—Ç–∏—è
        """
        corrections = []
        resolutions = []
        additions = []
        number_in_pattern = 0
        for w, word in enumerate(line.words):
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π –º–µ–Ω—å—à–µ 2 —Å–ª–æ–≥–æ–≤.
            if len(word.syllables) == 0:
                continue
            if len(word.syllables) == 1:
                if pattern[number_in_pattern].lower() == "s" and word.syllables[0].stress == -1:
                    additions.append(StressCorrection(line_number, w, 0, word.text, word.syllables[0].vowel()))
                number_in_pattern += len(word.syllables)
                continue
            stress_count = word.count_stresses()
            for syllable in word.syllables:
                if stress_count == 0 and pattern[number_in_pattern].lower() == "s":
                    # –£–¥–∞—Ä–µ–Ω–∏–π –Ω–µ—Ç, —Å—Ç–∞–≤–∏–º —Ç–∞–∫–æ–µ, –∫–∞–∫–æ–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –º–µ—Ç—Ä—É. –í–æ–∑–º–æ–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ.
                    additions.append(StressCorrection(line_number, w, syllable.number, word.text, syllable.vowel()))
                elif pattern[number_in_pattern].lower() == "u" and syllable.stress != -1:
                    # –£–¥–∞—Ä–µ–Ω–∏–µ –µ—Å—Ç—å –∏ –æ–Ω–æ –ø–∞–¥–∞–µ—Ç –Ω–∞ —ç—Ç–æ—Ç —Å–ª–æ–≥, –ø—Ä–∏ —ç—Ç–æ–º –≤ —à–∞–±–ª–æ–Ω–µ –±–µ–∑—É–¥–∞—Ä–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è.
                    # –ù–∞–π–¥—ë–º —Ç–∞–∫–æ–π —Å–ª–æ–≥, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –≤ —à–∞–±–ª–æ–Ω–µ —É–¥–∞—Ä–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è. –≠—Ç–æ –∏ –µ—Å—Ç—å –Ω–∞—à–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.
                    for other_syllable in word.syllables:
                        other_number_in_pattern = other_syllable.number - syllable.number + number_in_pattern
                        if syllable.number == other_syllable.number or pattern[other_number_in_pattern].lower() != "s":
                            continue
                        ac = StressCorrection(line_number, w, other_syllable.number, word.text, other_syllable.vowel())
                        if stress_count == 1 and other_syllable.stress == -1:
                            corrections.append(ac)
                        else:
                            resolutions.append(ac)
                number_in_pattern += 1
        return corrections, resolutions, additions

    @staticmethod
    def get_improved_markup(markup: Markup, result: ClassificationResult) -> Markup:
        """
        –£–ª—É—á—à–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É –ø–æ—Å–ª–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ—Ç—Ä–∞.

        :param markup: –Ω–∞—á–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.
        :param result: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        :return: —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.
        """
        for pos in result.corrections[result.metre] + result.resolutions[result.metre]:
            syllables = markup.lines[pos.line_number].words[pos.word_number].syllables
            for i, syllable in enumerate(syllables):
                syllable.stress = -1
                if syllable.number == pos.syllable_number:
                    syllable.stress = syllable.begin + get_first_vowel_position(syllable.text)
        for pos in result.additions[result.metre]:
            syllable = markup.lines[pos.line_number].words[pos.word_number].syllables[pos.syllable_number]
            syllable.stress = syllable.begin + get_first_vowel_position(syllable.text)

        return markup

    @staticmethod
    def improve_markup(markup: Markup) -> \
            Tuple[Markup, ClassificationResult]:
        """
        –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ç–∫–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º.

        :param markup: –Ω–∞—á–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.
        """
        result = MetreClassifier.classify_metre(markup)
        improved_markup = MetreClassifier.get_improved_markup(markup, result)
        return improved_markup, result


class StressPredictorAdapter:
    def __init__(self, accentuator):
        self.accentuator = accentuator

    def predict(self, word):
        return [int(self.accentuator.predict_stressed_charpos(word.lower()))]


class MetreClassifierAdapter:
    def __init__(self, accentuator):
        self.stress_predictor = StressPredictorAdapter(accentuator)

    def predict(self, text):
        markup = Markup.process_text(text, self.stress_predictor)
        m = MetreClassifier.classify_metre(markup)
        return m


if __name__ == '__main__':
    from phonetic import Accents, rhymed

    tmp_dir = '../../tmp'
    accentuator = Accents()
    accentuator.load_pickle(os.path.join(tmp_dir, 'accents.pkl'))
    accentuator.after_loading(stress_model_dir=os.path.join(tmp_dir, 'stress_model'))

    mclassifier = MetreClassifierAdapter(accentuator)

    text = """–ù–æ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ —Ç—ã, –ø–æ–ª–Ω–æ–≤–æ–¥–Ω–æ–µ —á—É–¥–æ
–ß—Ç–æ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–æ –º–µ–Ω—è –Ω–∞ –ø—É—Ç–∏
–ò –Ω–µ –¥–æ—à–ª–æ –¥–æ –º–µ–Ω—è, –∏–∑ –Ω–∏–æ—Ç–∫—É–¥–∞
–¢—É–¥–∞, –≥–¥–µ —Ç–≤–æ—ë –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–π—Ç–∏"""

    m = mclassifier.predict(text)
    print(m)

